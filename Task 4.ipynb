{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b6d6fc-62a0-4c44-9d60-e4478ad1e152",
   "metadata": {},
   "source": [
    "This task implements and compares both the policy and value iteration versions of dynamic programming for finding an optimal policy in an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rl-gym-maze environment from GitHub.\n",
    "!pip install gym\n",
    "!git clone https://github.com/AngusMaiden/rl-gym-maze\n",
    "!pip install -e ./rl-gym-maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if using this notebook in Google Colab. Restarts the runtime after installing rl-gym-maze.\n",
    "import os\n",
    "\n",
    "def restart_runtime():\n",
    " os.kill(os.getpid(), 9)\n",
    "\n",
    "restart_runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637486e1-76b4-4e6f-af96-fa5b972dbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Invoke the model maze environment  from Task 1.2C.\n",
    "env = gym.make('rl_gym_maze:rl-gym-maze-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b8974-685c-4859-a15c-a718d4c8984c",
   "metadata": {},
   "source": [
    "### Initialising variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ca6809-118b-40e3-be8b-27ac6a4ef17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise environment.\n",
    "env.reset()\n",
    "\n",
    "# Initialise dictionary to store flattened index of (x,y) coordinates.\n",
    "states = {}\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        states[i, j] = count\n",
    "        count+=1\n",
    "\n",
    "# Initialise max_iterations and gamma.\n",
    "max_iterations = 1000\n",
    "gamma = 1\n",
    "\n",
    "# Initialise other variables.\n",
    "next_state = np.zeros([4,2])\n",
    "next_reward = np.zeros(4)\n",
    "next_done = np.zeros(4, dtype=bool)\n",
    "next_V = np.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c106eff-4ec8-4b29-9e81-eac898dbb75d",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c234a42-6750-47f5-8497-c9adb6032c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation():\n",
    "    \n",
    "    # Initalise sweep counter.\n",
    "    sweeps = 0\n",
    "    \n",
    "    # Policy evluation limited to 1000 sweeps (limited to save computational time).\n",
    "    for i in range(max_iterations):\n",
    "    \n",
    "        # Initialise delta and exit variables.\n",
    "        delta = 0\n",
    "        exit = np.zeros(100, dtype=bool)\n",
    "        \n",
    "        # Initalise state.\n",
    "        env.reset()\n",
    "        \n",
    "        # Iterate though each state.\n",
    "        for x in range(10):\n",
    "            for y in range(10):\n",
    "                 \n",
    "                # Get flattened current state.\n",
    "                current_state = states[x, y]\n",
    "                \n",
    "                # Initalise v.\n",
    "                v = 0\n",
    "            \n",
    "                # Loop over each action.\n",
    "                for a in range(4):\n",
    "                \n",
    "                    # Populate a list of successor states and rewards for each eaction.\n",
    "                    next_state[a], next_reward[a], next_done[a], info = env.step(a)\n",
    "                    next_V[a] = states[next_state[a][0], next_state[a][1]]\n",
    "                \n",
    "                    # Calculate V(s) using Bellman equation:\n",
    "                    v += 0.25 * (next_reward[a] + gamma * V[int(next_V[a])])\n",
    "                \n",
    "                    # This is a lookahead, so reset state to before the step.\n",
    "                    env.location = (x,y)\n",
    "            \n",
    "                # Get delta value, which is the change in V(s).\n",
    "                delta = max(delta, np.abs(V[current_state] - v))\n",
    "            \n",
    "                # Update V(s) with calculated value.\n",
    "                V[current_state] = v\n",
    "            \n",
    "                # The state value of the terminal state is always 0.\n",
    "                V[99] = 0\n",
    "                \n",
    "                # If the change is state value is sufficiently small, exit the loop.\n",
    "                if delta < 0.1:\n",
    "                    exit[current_state] = True\n",
    "                \n",
    "                # Update environment state to next in sweep.\n",
    "                env.location = (x,y+1)\n",
    "                if env.location[1] == 10:\n",
    "                    env.location = (x+1, 0)\n",
    "                \n",
    "        # Increment episode counter and display progress indicator.\n",
    "        sweeps += 1\n",
    "        print(f'\\rPolicy Evaluation: Sweep {sweeps}', end='')\n",
    "        \n",
    "        if exit.all():\n",
    "            # Print the results and return the new V table.\n",
    "            print(f'\\r==Policy Evaluation==                 \\n'\n",
    "                  f'Converged in {sweeps} sweeps.\\n')\n",
    "            return V\n",
    "    \n",
    "    # Print the results and return the new V table.\n",
    "    print(f'\\r==Policy Evaluation==                         \\n'\n",
    "          f'Stopped after {sweeps} sweeps.\\n')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebeea33-1ebe-4460-b36d-c51a793a8174",
   "metadata": {},
   "source": [
    "### One step look-ahead function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4a68e7-53d0-4c2b-a645-cfec307049d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_values(state):\n",
    "    \n",
    "    # Check that 'state' argument is a tuple, for environment compatability.\n",
    "    assert isinstance(state, tuple), '\"state\" argument should be a tuple'\n",
    "    \n",
    "    # Initialise Q.\n",
    "    Q = np.zeros(4)\n",
    "    \n",
    "    # Set environment state to 'state' argument.\n",
    "    env.location = state\n",
    "    \n",
    "    # Loop over each action.\n",
    "    for a in range(4):\n",
    "           \n",
    "        # Populate a list of successor states and rewards for each eaction.\n",
    "        next_state[a], next_reward[a], next_done[a], info = env.step(a)\n",
    "        next_V[a] = states[next_state[a][0], next_state[a][1]]\n",
    "                \n",
    "        # Update Q(s,a) for each a using Bellman equation:\n",
    "        Q[a] = (next_reward[a] + gamma * V[int(next_V[a])])\n",
    "                \n",
    "        # This is a lookahead, so reset state to before the step.\n",
    "        env.location = state\n",
    "    \n",
    "    # Return a 4-tuple of the action-values at 'state'.\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08dd1d8-6a53-420b-96f9-f3977fb0bd22",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafd7567-9ef3-41a8-b7f8-2ef38a93a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Running Initial Random Policy==\n",
      "Initial Policy     | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Policy Evaluation==                         \n",
      "Stopped after 1000 sweeps.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Policy Iteration: 1 | Total Reward: -48 | Finished Maze?: Yes\n",
      "\n",
      "==Policy Evaluation==                         \n",
      "Stopped after 1000 sweeps.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Policy Iteration: 2 | Total Reward: -31 | Finished Maze?: Yes\n",
      "\n",
      "==Policy Evaluation==                         \n",
      "Stopped after 1000 sweeps.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Policy Iteration: 3 | Total Reward: -18 | Finished Maze?: Yes\n",
      "\n",
      "==Policy Evaluation==                         \n",
      "Stopped after 1000 sweeps.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Policy Iteration: 4 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Policy Evaluation==                         \n",
      "Stopped after 1000 sweeps.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Policy Iteration: 5 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Policy Evaluation==                         \n",
      "Stopped after 1000 sweeps.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "**Converged to optimal policy in 6 iterations.\n",
      "**\n",
      "==Running Optimised Policy==\n",
      "Policy Iteration: 6 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Grid view of state values==\n",
      "\n",
      "[[-1380.39 -1376.4  -1368.41 -1356.43 -1340.46 -1323.66 -1294.89 -1254.12 -1230.25 -1198.88]\n",
      " [-1505.85 -1512.17 -1514.49 -1512.82 -1337.3  -1331.65 -1302.87 -1233.25 -1233.75 -1163.52]\n",
      " [-1495.54 -1467.89 -1489.52 -1507.16 -1330.13 -1335.65 -1306.87 -1207.87 -1128.66 -1124.17]\n",
      " [-1481.25 -1442.26 -1519.14 -1515.15 -1318.98 -1322.97 -1194.48 -1178.5  -1129.16 -1076.33]\n",
      " [-1462.95 -1412.65 -1379.04 -1341.44 -1299.84 -1087.81 -1206.46 -1214.45 -1218.44  -971.65]\n",
      " [-1440.67 -1452.65 -1460.63 -1464.63 -1235.11 -1083.81 -1046.64 -1005.47  -960.3   -862.97]\n",
      " [-1402.41 -1243.93 -1222.08 -1196.22 -1166.38 -1113.   -1084.81 -1052.63 -1008.46  -652.98]\n",
      " [-1360.17 -1261.8  -1273.78 -1281.77 -1117.18 -1064.   -1064.61 -1060.62  -184.47  -438.98]\n",
      " [-1313.92 -1263.69 -1211.35 -1285.76 -1012.59 -1006.81  -644.89  -491.42  -180.47  -220.99]\n",
      " [-1166.99 -1163.   -1155.01 -1086.69 -1014.38  -939.85  -794.37  -333.94  -172.47     0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Define initial policy, P, as random (p(s,a) = 0.25 for all s).\n",
    "P = np.ones([100, 4]) / 4\n",
    "\n",
    "# Initialise V table.\n",
    "V = np.zeros(100)\n",
    "\n",
    "# Initialise iterations counter.\n",
    "iterations = 0\n",
    "\n",
    "# Initialise lists to hold episode rewards and maze completions.\n",
    "policy_iteration_rewards = []\n",
    "policy_iteration_finished = []\n",
    "\n",
    "# Initialise 'stable' value in each state to determine if policy is optmised.\n",
    "stable = np.zeros(100, dtype=bool)\n",
    "\n",
    "# Policy iteration limited to 100 iterations (hoping to optimise before limit).\n",
    "for i in range(100):\n",
    "    \n",
    "    # Run an episode at the beginning of each loop using the current policy.\n",
    "    # and calculate the total episode reward and percentage maze completions.\n",
    "    if iterations == 0:\n",
    "        print(f'==Running Initial Random Policy==')\n",
    "    else:\n",
    "        print(f'==Running Improved Policy==')\n",
    "    \n",
    "    # Initalise environment and other variables.\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Limit episode to 100 steps (hoping to finish the task before limit).\n",
    "    for step in range(100):\n",
    "    \n",
    "        # Get flattened current state.\n",
    "        current_state = states[state[0],state[1]]\n",
    "    \n",
    "        # Select next action based on optimal policy.\n",
    "        action = int(np.random.choice(np.flatnonzero(P[current_state] == P[current_state].max())))\n",
    "    \n",
    "        # Get state, reward and done variables from environment 'step' function.\n",
    "        state, reward, done, info = env.step(action)\n",
    "    \n",
    "        # Update counter and reward tracking variables.\n",
    "        counter += 1\n",
    "        total_reward += reward\n",
    "    \n",
    "        # Display current location as agent moves through maze.\n",
    "        print(f'\\rStep: {counter} | Location: {state} | Reward: {reward}', end = '')\n",
    "    \n",
    "        # If we reach the goal, end the episode.\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Create a variable for calculating and displaying 'Finished maze' statistics.\n",
    "    if done:\n",
    "        finished = 'Yes'\n",
    "    else:\n",
    "        finished = 'No'\n",
    "    \n",
    "    # Display run results.\n",
    "    if iterations == 0:\n",
    "        print(f'\\rInitial Policy     | Total Reward: {total_reward} | '\n",
    "              f'Finished Maze?: {finished}\\n')\n",
    "    else:\n",
    "        print(f'\\rPolicy Iteration: {iterations} | Total Reward: {total_reward} | '\n",
    "              f'Finished Maze?: {finished}\\n')\n",
    "       \n",
    "    # Get V table from policy evaluation.\n",
    "    V = policy_evaluation()\n",
    "    \n",
    "    # Policy Improvement.\n",
    "    print(f'==Policy Improvement==\\n'\n",
    "          f'Updated policy at each state to be greedy.\\n')\n",
    "\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "            \n",
    "            # Get flattened current state.\n",
    "            current_state = states[x, y]\n",
    "            \n",
    "            # Select the best action (argmax) under current policy, breaking ties randomly.\n",
    "            old_action = int(np.random.choice(np.flatnonzero(P[current_state] == P[current_state].max())))\n",
    "            \n",
    "            # Get next action values at current state.\n",
    "            Q = action_values((x,y))\n",
    "            \n",
    "            # Select best action (argmax) based on new action values, breaking ties randomly.\n",
    "            new_action = int(np.random.choice(np.flatnonzero(Q == Q.max())))\n",
    "            \n",
    "            # Check if action under the new policy is different than the old.\n",
    "            # If they are different, update the policy to take the best action.\n",
    "            if old_action == new_action:\n",
    "                stable[current_state] = True\n",
    "            else:\n",
    "                P[current_state] = np.eye(4)[new_action]\n",
    "                stable[current_state] = False\n",
    "            \n",
    "            # Update environment state to next in sweep.\n",
    "            env.location = (x,y+1)\n",
    "            if env.location[1] == 10:\n",
    "                env.location = (x+1, 0)\n",
    "    \n",
    "    # Increment iteration counter and display progress indicator.\n",
    "    iterations += 1\n",
    "    \n",
    "    # Update episode rewards and maze completions lists.\n",
    "    policy_iteration_rewards.append(total_reward)\n",
    "    policy_iteration_finished.append(finished)\n",
    "    \n",
    "    # If no change in policy, we have an optimal policy (exit the loop).\n",
    "    if stable.all():\n",
    "        print(f'**Converged to optimal policy in {iterations} iterations.\\n**')\n",
    "        break\n",
    "\n",
    "# Run an episode with optimised policy.\n",
    "print(f'==Running Optimised Policy==')\n",
    "    \n",
    "# Initalise environment and other variables.\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0\n",
    "total_reward = 0\n",
    "    \n",
    "# Limit episode to 100 steps (hoping to finish the task before limit).\n",
    "for step in range(100):\n",
    "    \n",
    "    # Get flattened current state.\n",
    "    current_state = states[state[0],state[1]]\n",
    "    \n",
    "    # Select next action based on optimal policy.\n",
    "    action = int(np.random.choice(np.flatnonzero(P[current_state] == P[current_state].max())))\n",
    "    \n",
    "    # Get state, reward and done variables from environment 'step' function.\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update counter and reward tracking variables.\n",
    "    counter += 1\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Display current location as agent moves through maze.\n",
    "    print(f'\\rStep: {counter} | Location: {state} | Reward: {reward}', end = '')\n",
    "  \n",
    "    # If we reach the goal, end the episode.\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "# Create a variable for calculating and displaying 'Finished maze' statistics.\n",
    "if done:\n",
    "    finished = 'Yes'\n",
    "else:\n",
    "    finished = 'No'\n",
    "    \n",
    "# Display run results.\n",
    "print(f'\\rPolicy Iteration: {iterations} | Total Reward: {total_reward} | '\n",
    "      f'Finished Maze?: {finished}\\n')\n",
    "\n",
    "policy_iteration_rewards.append(total_reward)\n",
    "policy_iteration_finished.append(finished)\n",
    "\n",
    "print(f'==Grid view of state values==\\n')\n",
    "np.set_printoptions(precision = 2, suppress = True, linewidth = 100)\n",
    "print(V.reshape([10,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c77d019-e6bf-422b-a7f4-fbeabed5b254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results.\n",
    "plt.plot(policy_iteration_rewards)\n",
    "plt.title('Policy Iteration Performance')\n",
    "plt.xticks(ticks=np.arange(0,iterations))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f60ee-2244-4867-847b-f606d0a7c91f",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e19cc82-aa8e-4f17-8682-2e4e8f58b901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Running Initial Random Policy==\n",
      "Initial Policy     | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 1.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 1 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 2.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 2 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 3.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 3 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 4.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 4 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 5.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 5 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 6.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 6 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 7.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 7 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 8.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 8 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 9.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 9 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 10.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 10 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 11.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 11 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 12.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 12 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 13.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 13 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 14.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 14 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 15.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 15 | Total Reward: -100 | Finished Maze?: No\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 16.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 16 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 17.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 17 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 18.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 18 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 19.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 19 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Value Iteration==                   \n",
      "Iteration 20.\n",
      "\n",
      "==Policy Improvement==\n",
      "Updated policy at each state to be greedy.\n",
      "\n",
      "==Running Improved Policy==\n",
      "Iteration: 20 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "==Value Iteration==                   \n",
      "**Converged to optimal policy in 21 iterations**.\n",
      "\n",
      "==Running Optimised Policy==\n",
      "Policy Iteration: 21 | Total Reward: -17 | Finished Maze?: Yes\n",
      "\n",
      "Grid view of the state values:\n",
      "\n",
      "[[-17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.]\n",
      " [-18. -19. -20. -19. -14. -13. -12.  -9. -10.  -7.]\n",
      " [-17. -16. -17. -18. -13. -14. -13.  -8.  -7.  -6.]\n",
      " [-16. -15. -20. -19. -12. -13.  -8.  -7.  -6.  -5.]\n",
      " [-15. -14. -13. -12. -11.  -8.  -9. -10. -11.  -4.]\n",
      " [-14. -15. -16. -17. -10.  -7.  -6.  -5.  -4.  -3.]\n",
      " [-13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -2.]\n",
      " [-12. -11. -12. -13.  -8.  -7.  -8.  -7.  -2.  -1.]\n",
      " [-11. -10.  -9. -14.  -7.  -6.  -3.  -2.  -1.   0.]\n",
      " [-10.  -9.  -8.  -7.  -6.  -5.  -4.  -1.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Define initial policy, P, as random (p(s,a) = 0.25 for all s).\n",
    "P = np.ones([100, 4]) / 4\n",
    "\n",
    "# Initialise V table.\n",
    "V = np.zeros(100)\n",
    "\n",
    "# Initialise iterations counter.\n",
    "iterations = 0\n",
    "\n",
    "value_iteration_rewards = []\n",
    "value_iteration_finished = []\n",
    "    \n",
    "# Initialise 'stable' value in each state to check if value is optimised.\n",
    "stable = np.zeros(100, dtype=bool)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # Run an episode at the beginning of each loop using the current policy.\n",
    "    # and calculate the total episode reward and percentage maze completions.\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(f'==Running Initial Random Policy==')\n",
    "    else:\n",
    "        print(f'==Running Improved Policy==')\n",
    "    \n",
    "    # Initalise environment and other variables.\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Limit episode to 100 steps (hoping to finish the task before limit).\n",
    "    for step in range(100):\n",
    "    \n",
    "        # Get flattened current state.\n",
    "        current_state = states[state[0],state[1]]\n",
    "    \n",
    "        # Select next action based on optimal policy.\n",
    "        action = int(np.random.choice(np.flatnonzero(P[current_state] == P[current_state].max())))\n",
    "    \n",
    "        # Get state, reward and done variables from environment 'step' function.\n",
    "        state, reward, done, info = env.step(action)\n",
    "    \n",
    "        # Update counter and reward tracking variables.\n",
    "        counter += 1\n",
    "        total_reward += reward\n",
    "    \n",
    "        # Display current location as agent moves through maze.\n",
    "        print(f'\\rStep: {counter} | Location: {state} | Reward: {reward}', end = '')\n",
    "    \n",
    "        # If we reach the goal, end the episode.\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Create a variable for calculating and displaying 'Finished maze' statistics.\n",
    "    if done:\n",
    "        finished = 'Yes'\n",
    "    else:\n",
    "        finished = 'No'\n",
    "    \n",
    "    # Display run results.\n",
    "    if iterations == 0:\n",
    "        print(f'\\rInitial Policy     | Total Reward: {total_reward} | '\n",
    "              f'Finished Maze?: {finished}\\n')\n",
    "    else:\n",
    "        print(f'\\rIteration: {iterations} | Total Reward: {total_reward} | '\n",
    "              f'Finished Maze?: {finished}\\n')\n",
    "    \n",
    "    # Initialise delta and exit variables.\n",
    "    delta = 0\n",
    "    \n",
    "    # Value iteration.\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "            \n",
    "            # Get flattened current state.\n",
    "            current_state = states[x, y]\n",
    "         \n",
    "            # Update v with current state value.\n",
    "            v = V[current_state]\n",
    "            \n",
    "            # Get next action values at current state.\n",
    "            Q = action_values((x,y))\n",
    "            \n",
    "            # Update state value with best action value.\n",
    "            V[current_state] = np.max(Q)\n",
    "            \n",
    "            V[99] = 0\n",
    "            \n",
    "            # Caclulate change in value.\n",
    "            delta = max(delta, np.abs(v - V[current_state]))\n",
    "            \n",
    "            # If the change is state value is sufficiently small, exit the loop.\n",
    "            if delta < 0.1:\n",
    "                stable[current_state] = True\n",
    "            else:\n",
    "                stable[current_state] = False\n",
    "            \n",
    "            # Update environment state to next in sweep.\n",
    "            env.location = (x,y+1)\n",
    "            if env.location[1] == 10:\n",
    "                env.location = (x+1, 0)\n",
    "\n",
    "    # Increment iteration counter and display progress indicator.\n",
    "    iterations += 1\n",
    "    \n",
    "    # Update episode rewards and maze completions lists.\n",
    "    value_iteration_rewards.append(total_reward)\n",
    "    value_iteration_finished.append(finished)        \n",
    "    \n",
    "    if stable.all():\n",
    "        # Print the results and exit the loop.\n",
    "        print(f'\\r==Value Iteration==                   \\n'\n",
    "              f'**Converged to optimal policy in {iterations} iterations**.\\n')\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        print(f'\\r==Value Iteration==                   \\n'\n",
    "              f'Iteration {iterations}.\\n')\n",
    "    \n",
    "    # Define a deterministic policy, P, using the latest V table.\n",
    "    print(f'==Policy Improvement==\\n'\n",
    "          f'Updated policy at each state to be greedy.\\n')\n",
    "\n",
    "    # Sweep through each state.\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "        \n",
    "            # Get flattened current state.\n",
    "            current_state = states[x,y]\n",
    "       \n",
    "            # Get next action values at current state.\n",
    "            Q = action_values((x,y))\n",
    "        \n",
    "            # Select best action (argmax) based on optimised action values, breaking ties randomly.\n",
    "            next_action = int(np.random.choice(np.flatnonzero(Q == Q.max())))\n",
    "        \n",
    "            # Update the policy at this state to be greedy for the best action.\n",
    "            P[current_state] = np.eye(4)[next_action]\n",
    "            \n",
    "            # Update environment state to next in sweep.\n",
    "            env.location = (x,y+1)\n",
    "            if env.location[1] == 10:\n",
    "                env.location = (x+1, 0)\n",
    "\n",
    "# Run an episode with optimised policy.\n",
    "print(f'==Running Optimised Policy==')\n",
    "    \n",
    "# Initalise environment and other variables.\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0\n",
    "total_reward = 0\n",
    "    \n",
    "# Limit episode to 100 steps (hoping to finish the task before limit).\n",
    "for step in range(100):\n",
    "    \n",
    "    # Get flattened current state.\n",
    "    current_state = states[state[0],state[1]]\n",
    "    \n",
    "    # Select next action based on optimal policy.\n",
    "    action = int(np.random.choice(np.flatnonzero(P[current_state] == P[current_state].max())))\n",
    "    \n",
    "    # Get state, reward and done variables from environment 'step' function.\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update counter and reward tracking variables.\n",
    "    counter += 1\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Display current location as agent moves through maze.\n",
    "    print(f'\\rStep: {counter} | Location: {state} | Reward: {reward}', end = '')\n",
    "  \n",
    "    # If we reach the goal, end the episode.\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "# Create a variable for calculating and displaying 'Finished maze' statistics.\n",
    "if done:\n",
    "    finished = 'Yes'\n",
    "else:\n",
    "    finished = 'No'\n",
    "    \n",
    "# Display run results.\n",
    "print(f'\\rPolicy Iteration: {iterations} | Total Reward: {total_reward} | '\n",
    "      f'Finished Maze?: {finished}\\n')\n",
    "\n",
    "# Update episode rewards and maze completions lists.\n",
    "value_iteration_rewards.append(total_reward)\n",
    "value_iteration_finished.append(finished)\n",
    "\n",
    "print(f'Grid view of the state values:\\n')\n",
    "np.set_printoptions(precision = 2, suppress = True, linewidth = 100)\n",
    "print(V.reshape([10,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658f9374-21fb-4c7e-b6f4-274e0a731986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAk5ElEQVR4nO3deZxcVZ338c833dlDFkiQJcEAEhQYRQgMOopsI+g4ZOBxyYzjIOigCCo46oj4uPO8XFGfWcQIDqK44AgKjsiiAuKIkDAEiICGTcIudIWQCuklv/njngpFp6v6VtPV91b6+3696tV3O/f+aun61bnnnnMVEZiZmeUxoegAzMysczhpmJlZbk4aZmaWm5OGmZnl5qRhZma5OWmYmVluTho2ZiSFpBcUHUc7SFol6ZCi42hG0lRJl0paK+kHRcdjnclJw3KTdLmkTw6xfImkhyV1FxTX1ZLenqYPkbSmzcc7T9Kn65dFxN4RcXUbjnW1pKclPSXpT5IukrTjCHf3euB5wHYR8YZRDNPGEScNa8V5wFskadDytwAXRET/2Ic0uopKfMM4JSJmAIuA2cCXWt2BpC7g+cDvR/I+lfR1sQI4aVgrfgRsC7yytkDSHOB1wPmSDpT0G0kVSQ9J+ldJk4baUX3tIM2/VdJ1dfMvlHSlpCck3SnpjcMFJ2k6cBmwU/pl/pSknSRNkPQhSXdJelzShZK2TWUWptNmb5P0R+AXafkPUu1praRrJe2dlp8IvBn4YNr/pWn5vZKOSNOTJX1Z0oPp8WVJk9O6QyStkfRPkh5Nr9PxeV78iHgC+CGwz3CvUaoNfVXSTyWtB64FPgq8KcX9tvS6fETSfSmW8yXNavS6pPfo15K+lN7juyW9PC2/P+3juLoY/krS/0h6Mq3/eN262v6Pk/THVIs6o259l6QPp/dsnaQVkhaM9LNhoygi/PAj9wP4OnBO3fw7gJvT9P7AQUA3sBC4HTi1btsAXpCmrwbeXrfurcB1aXo6cD9wfNrXfsCfgL0bxLR5X8AhwJpB608FrgfmA5OBrwHfTesWprjOT8edmpafAGyTtv9y7TmmdecBnx50jHuBI9L0J9PxtgfmAf8NfKouvv60zUTgtUAVmJPjuc0lS2rfGu41SjGuBf6C7MfhFODjwLfr9n0CsBrYDZgBXAR8q9Hrkt6j/nTMLuDTwB+Bf0uv06uBdcCMuuf6Z+n4LwYeAf5m0P6/nvb9EmAj8KK0/gPArcCegNL67YZ73n6MwXdA0QH40VkP4BXpy6j25fpr4LQG254KXFw3nzdpvAn41aB9fQ34WIPj1H+xHsKWSeN24PC6+R2BPp5JbgHs1uQ5z07bzErz59E8adwFvLZu3ZHAvXXxbQC669Y/ChzU5LlVgQrwAHABWSJq+hqlGM8ftP7jPDtp/Bx4V938ns1el/Qe/aFu/s/SNs+rW/Y4sG+D5/Jl4Etpurb/+XXrbwCWpuk7gSVD7KOlz4Yfo//weUprSURcJ+kxYImkG4ADgGMBJC0CzgIWA9PIvnxWjOAwzwf+XFKlblk32S/skXg+cLGkTXXLBsgahWvur02k8/9nAm8g+4KulZtLljCHsxNwX938fWlZzePx7HaFKtkv/UbeExHn1C+QlOc1up/mhoqzmwavS/JI3fQGgIgYvGxGivHPgc+QnU6bRFYbGXzV1sN10/WvwwKy5DvYaH82rEVu07CROB/4B7IG8CvqvjS+CtwB7BERM4EPk51aGMp6ssRSs0Pd9P3ANRExu+4xIyJOyhHbUMM23w+8ZtD+pkTEAw3K/R2wBDgCmEX2q5i65zLc0NAPkn251eySlo2mPK/RSOLs59mJ4bkMg/0d4BJgQUTMAs6m8edhsPuB3RssH+lnw0aBk4aNxPlkX6j/CHyzbvk2wJPAU5JeCDT7R74ZOFbSNGV9N95Wt+4nwCJJb5E0MT0OkPSiHLE9AmxXa9BNzgbOTL/OkTRP0pIm+9iG7Pz642SJ7f8NcYzdmpT/LvCRdJy5ZA3Q384Reyuey2tUH+dpknaVNIPseX4/Ru8quG2AJyLiaUkHkiXjvM4BPiVpD2VeLGk7Rud523PgpGEti4h7yRp3p5P9kqx5P9kXwzqyBs7vN9nNl4Besi/gb5Kdq6/tfx1Zo+pSsl/DDwOfJTu9MVxsd5B9Gd6drvDZCfhKivMKSevIGqn/vMluzic7VfMA8Lu0fb1zgb3S/n80RPlPA8uBW8gac29Ky0bNc3mN6nyD7LTOtcA9wNPAu0cxzHcBn0yv+UeBC1soe1ba/gqyHyLnkrWjjcbztudAqSHJzMxsWK5pmJlZbk4aZmaWm5OGmZnl5qRhZma5bfWd++bOnRsLFy4sOgwzs46yYsWKP0XEvMHLt/qksXDhQpYvX150GGZmHUXSfUMt9+kpMzPLzUnDzMxyc9IwM7PcnDTMzCw3Jw0zM8vNScPMzHJz0jAzs9y2+n4aZrb1uezWh7j9oSeLDqP03n34HkzsGt26gZOGmXWcf/rBSqq9AyjvfQDHqXcd+gImdo3uPp00zKyjPN03QLV3gA8cuScnH/qCosMZd9ymYWYdpVLtA2D2tIkFRzI+OWmYWUfpqfYCMGfapIIjGZ9KlzQkfV7SHZJukXSxpNl1606XtFrSnZKOLDBMMytILWm4plGM0iUN4Epgn4h4MfB74HQASXuR3Ux+b+Ao4N8ljXITj5mVXe30lGsaxShd0oiIKyKiP81eD8xP00uA70XExoi4B1gNHFhEjGZWHLdpFKt0SWOQE4DL0vTOwP1169akZVuQdKKk5ZKWP/bYY20O0czGkts0ilXIJbeSrgJ2GGLVGRHx47TNGUA/cEGt2BDbx1D7j4hlwDKAxYsXD7mNmXWmSrWXKRMnMGW0OyBYLoUkjYg4otl6SccBrwMOj4jal/4aYEHdZvOBB9sToZmVVU+1z7WMApXu9JSko4B/Bo6OiGrdqkuApZImS9oV2AO4oYgYzaw4lWofs500ClPGHuH/CkwGrlQ2RsD1EfHOiFgl6ULgd2SnrU6OiIEC4zSzAlSqvcye6kbwopQuaUREw3EBIuJM4MwxDMfMSqan2sueO2xTdBjjVulOT5mZNePTU8Vy0jCzjhERVDb0Mcd9NArjpGFmHWPdxn4GNgWzp7qmURQnDTPrGJX17g1eNCcNM+sY7g1ePCcNM+sYm5PGdNc0iuKkYWYdY+2G7PTULLdpFMZJw8w6Rs/62ukp1zSK4qRhZh2jp1qraThpFMVJw8w6RqXay8wp3XR3+aurKH7lzaxjVDb0MWe62zOK5KRhZh2jp9rnwQoL5qRhZh2jUu31uFMFc9Iws47RU+31lVMFc9Iws47hEW6L56RhZh2hf2AT657u97hTBXPSMLOOUEm9wT3uVLGcNMysI1TSuFOuaRTLScPMOkKtN7hrGsVy0jCzjlCp+l4aZeCkYWYdwffSKAcnDTPrCG7TKAcnDTPrCD3VProniBmTu4sOZVxz0jCzjlDr2Cep6FDGNScNM+sI2bhTPjVVNCcNM+sIHneqHJw0zKwjeNypcnDSMLOOUKn2uaZRAk4aZtYRenwvjVJw0jCz0tvQO8DG/k1uCC8BJw0zKz33Bi8PJw0zK71nkoZrGkVz0jCz0lubBiucNdU1jaI5aZhZ6W0eFn26axpFc9Iws9Jzm0Z5OGmYWel5hNvyaDhcpKRLgWi0PiKObktEZmaDVKp9TJvUxeTurqJDGfeajTH8hfT3WGAH4Ntp/m+Be9sYk5nZs/RU+5g91bWMMmiYNCLiGgBJn4qIg+tWXSrp2rZHZmaWVNwbvDTytGnMk7RbbUbSrsC89oW0+TjvlxSS5tYtO13Sakl3Sjqy3TGYWTn0VHt95VRJ5LkF1qnA1ZLuTvMLgRPbFRCApAXAXwJ/rFu2F7AU2BvYCbhK0qKIGGhnLGZWvMqGPnacPbXoMIxhkoakCcAsYA/ghWnxHRGxsc1xfQn4IPDjumVLgO+lY98jaTVwIPCbNsdiZgWruE2jNJqenoqITcApEbExIlamR1sThqSjgQciYuWgVTsD99fNr0nLhtrHiZKWS1r+2GOPtSlSMxsLmzYFlWqv+2iURJ7TU1dKej/wfWB9bWFEPDHSg0q6iuyKrMHOAD4MvHqoYkMsG/KS4IhYBiwDWLx4ccPLhs2s/NY93c+mcB+NssiTNE5If0+uWxbAbkNsm0tEHDHUckl/BuwKrEw3j58P3CTpQLKaxYK6zecDD440BjPrDO4NXi7DJo2I2HUsAknHuhXYvjYv6V5gcUT8SdIlwHcknUXWEL4HcMNYxWZmxahsyMadck2jHPLUNJC0D7AXMKW2LCLOb1dQQ4mIVZIuBH4H9AMn+8ops61fz+YhRFzTKINhk4akjwGHkCWNnwKvAa4D2p40ImLhoPkzgTPbfVwzK4+K76VRKnk6970eOBx4OCKOB14CTG5rVGZmSc/6NCy6axqlkCdpbEiX3vZLmgk8ynNoBDcza0VlQx8SzHQ/jVLI06axXNJs4OvACuAp3ABtZmOkUu1l5pSJdE0Y6qp7G2t5rp56V5o8W9LPgJkRcUt7wzIzy/RU+9yeUSJ5GsLPB34F/Coi7mh/SGZmz/AIt+WSp03jPGBH4F8k3SXph5Le296wzMwyFdc0SiXP6alfSLoGOAA4FHgn2UizX2lzbGZm9FR7ecH2M4oOw5I8p6d+DkwnG032V8ABEfFouwMzM4M0wq1rGqWR5/TULUAvsA/wYmAfSR7Y3szarrd/E09t7HcfjRLJc3rqNABJM4Djgf8gG6HWHfzMrK0qG9wbvGzynJ46BXglsD9wH/ANstNUZmZttbaa9Qaf5ZpGaeTp3DcVOAtYERH9bY7HzGyznmptCBHXNMpi2DaNiPg8MBF4C4CkeZLGbLh0Mxu/fC+N8hk2aaRRbv8ZOD0tmgh8u51BmZnBMyPc+uqp8shz9dQxwNGkW71GxIPANu0MyswMssttwTWNMsmTNHojIkj345Y0vb0hmZlleqp9TOwS0yZ1FR2KJXmSxoWSvgbMlvSPwM+Bc9oblpnZM+NOSR7htizy9NP4gqS/BJ4E9gT+b0Rc2fbIzGzc66n2+sqpkmmaNCR1AXNSkrhS0iTgrZJuj4gXjUmEZjZuZUOIuD2jTBqenpK0FHgCuEXSNZIOBe4mu0f4m8coPjMbxyrVPmb7jn2l0qym8RFg/4hYLWk/sgELl0bExWMTmpmNdz3VXvZdMLvoMKxOs4bw3ohYDRARNwH3OGGY2ViJiKymMd01jTJpVtPYXtL76uZn1M9HxFntC8vMxrtq7wC9A5vcR6NkmiWNr/PsTnyD583M2qayIevY5zaNcmmYNCLiE2MZiJlZvZ71tSFEXNMokzyd+8zMxlzFI9yWkpOGmZXS5hFup7umUSZOGmZWSm7TKKc8Q6M/T9K5ki5L83tJelv7QzOz8aziNo1SylPTOA+4HNgpzf8eOLVN8ZiZAdkIt9MndTGp2ydEyiTPuzE3Ii4ENgGkW74OtDUqMxv3aiPcWrnkSRrrJW3HM/fTOAhY29aozGzcq2zoY457g5fOsEOjA+8DLgF2l/RrYB7w+rZGZWbjXk+1l9lTXdMomzz307hJ0qvI7qUh4M6I6Gt7ZGY2rlWqfew8e2rRYdggDZOGpGMbrFokiYi4qE0xmZmlGzC5plE2zWoaf53+bg+8HPhFmj8UuBpw0jCzthjYFKzd0Ofe4CXUbOyp4wEk/QTYKyIeSvM7Av82NuGZ2Xi07uk+ImCWaxqlk+fqqYW1hJE8AixqUzxmZvR43KnSypM0rpZ0uaS3SjoO+C/gl+0MStK7Jd0paZWkz9UtP13S6rTuyHbGYGbF2TzulGsapZPn6qlTJB0DHJwWLWvnHfzSvciXAC+OiI2Stk/L9wKWAnuT9U6/StKiiHBHQ7OtTKVaG0LENY2yydNPA+C/gX6yDn43tC8cAE4CPhMRGwEi4tG0fAnwvbT8HkmrgQPJ7l1uZluR2rDo7hFePnkGLHwjWaJ4PfBG4LeS2tm5bxHwSkm/lXSNpAPS8p2B++u2W5OWmdlWxm0a5ZWnpnEGcEDtF7+kecBVwH+O9KCSrgJ2aHCsbmAOcBBwAHChpN3IOhYOFg32fyJwIsAuu+wy0jDNrCCVai8TBDOnOGmUTZ6kMaHuFBHA4zzH+3BExBGN1kk6CbgoIgK4QdImYC5ZzWJB3abzgQcb7H8ZsAxg8eLFQyYWMyuvnmovs6ZOZMKEoX4rWpHyfPn/rO7qqbeSXT310zbG9CPgMABJi4BJwJ/Ixr9aKmmypF2BPWh/+4qZFaBS7fOVUyWV5+qpD6QhRV5BdoqorVdPAd8AviHpNqAXOC7VOlZJuhD4HVmj/Mm+csps61Sp9jHL7RmlNGzSkDQd+HFEXCRpT2BPSRPbNWhhRPQCf99g3ZnAme04rpmVR0+1l+fNnFJ0GDaEPKenrgUmS9qZrAH8eLK7+ZmZtUWl2uc+GiWVJ2koIqrAscC/RMQxwF7tDcvMxjOPcFteuZKGpJcBbyZrBIf8nQLNzFqysX+Aau8As6e6plFGeZLGqcDpwMURsSr1mWjr2FNmNn6trfUGn+6aRhnluXrqGuCauvm7gfe0MygzG7/cG7zcmt2578sRcaqkSxmi53VEHN3WyMxsXPIIt+XWrKbxrfT3C2MRiJkZPDNY4Sy3aZRSszv3rUh/r5E0CXghWY3jztSXwsxs1NWGRZ/jNo1SytO576+As4G7yHqE7yrpHRFxWbuDM7Pxx20a5Zbn0tkvAodGxGoASbuTXXrrpGFmo65S7WVS9wSmTuwqOhQbQp5Lbh+tJYzkbuDRRhubmT0X2WCFE5E8wm0Z5alprJL0U+BCsjaNNwA3pkEMiYiL2hifmY0zPdVeZk91e0ZZ5UkaU4BHgFel+ceAbYG/JksiThpmNmo87lS55encd/xYBGJmBllNY/d5M4oOwxpo2KaR7l1Rm/7soHVXtDMoMxu/Khv6mDPdNY2yatYQvkfd9F8OWjevDbGY2TgXEVSqvcxym0ZpNUsaze6t7ftum9moW987QN9AuI9GiTVr05gm6aVkiWVqmlZ6TB2L4MxsfOlZ73Gnyq5Z0ngIOCtNP1w3XZs3MxtVtXGnfPVUeTUbe+rQsQzEzKyyIatpzHZNo7Ty9Ag3MxsTHneq/Jw0zKw0aiPcuqZRXk4aZlYaPevdplF2ze7ct1+zghFx0+iHY2bjWWVDL9tM7mZil3/PllWzq6e+2GRdAIeNcixmNs5Vqn3Mci2j1Hz1lJmVRk+11300Si7PKLdI2gfYi2zEWwAi4vx2BWVm41OPR7gtvTy3e/0YcAhZ0vgp8BrgOsBJw8xG1dpqL8/fdlrRYVgTeVqbXg8cDjychkl/CTC5rVGZ2bjkmkb55UkaGyJiE9AvaSbZrV53a29YZjbeDGwKnny6z300Si5Pm8ZySbOBrwMrgKeAG9oZlJmNP2s39BHh3uBll+fOfe9Kk2dL+hkwMyJuaW9YZjbe9FQ9wm0nGPb0lKSf16Yj4t6IuKV+mZnZaKiNcOt+GuXWrEf4FGAaMFfSHLL7aADMBHYag9jMbBypuKbREZqdnnoHcCpZgqgfMuRJ4N/aGJOZjUMe4bYzNOsR/hXgK5LeHRH/MoYxmdk45BFuO0Oeq6e+Juk9wMFp/mrgaxHR17aozGzcqVT76JogZk7JNVCFFSTPu/PvwMT0F+AtwFeBt7crKDMbf3qqvcyaOhFJw29shWnWEN4dEf3AARHxkrpVv5C0sv2hmdl4UnFv8I7Q7JLbWge+AUm71xZK2g0YaFdAkvaVdL2kmyUtl3Rg3brTJa2WdKekI9sVg5mNPY9w2xmanZ6q1RHfD/xS0t1pfiFwfBtj+hzwiYi4TNJr0/whkvYClgJ7k13RdZWkRRHRtgRmZmOnUu1jp9lTht/QCtUsacyT9L40/TWgC1hPNjz6S4FftimmIOsLAjALeDBNLwG+FxEbgXskrQYOBH7TpjjMbAxVqr28aMeZw29ohWqWNLqAGTxT4yDNA2zTtoiyviGXS/oC2emzl6flOwPX1223Ji3bgqQTgRMBdtlll7YFamajp6fa5z4aHaBZ0ngoIj7ZjoNKugrYYYhVZ5ANw35aRPxQ0huBc4EjeHbyqomh9h8Ry4BlAIsXLx5yGzMrj6f7BtjQN8Cc6W7TKLs8bRqjLiKOaHhQ6XzgvWn2B8A5aXoNsKBu0/k8c+rKzDpYbdwpXz1Vfs2unjp8zKJ4tgeBV6Xpw4A/pOlLgKWSJkvaFdgDD9FutlWobEi9wae6plF2zYYReWIsA6nzj2TDl3QDT5PaJiJilaQLgd8B/cDJvnLKbOvQs97jTnWK0vXXj4jrgP0brDsTOHNsIzKzdvO4U50jz+1ezczaavMIt9Nd0yg7Jw0zK1ytTcM9wsvPScPMClep9jG5ewJTJnYVHYoNw0nDzArXs97jTnUKJw0zK1yPR7jtGE4aZla4tRtc0+gUThpmVjjXNDqHk4aZFa5S7XUfjQ7hpGFmhYoIKh7htmM4aZhZodZt7Kd/U7hNo0M4aZhZodam3uCzXNPoCE4aZlaonqp7g3cSJw0zK9Tmcadc0+gIThpmViiPcNtZnDTMrFC+a19ncdIws0LV2jRmT3XS6AROGmZWqEq1j22mdNPd5a+jTuB3ycwK1VP1uFOdxEnDzArl3uCdxUnDzApVqfYyyzWNjuGkYWaF6nFNo6M4aZhZodym0VmcNMysMP0Dm1j3dL/7aHQQJw0zK8zaDaljn/todAwnDTMrzOZxp6b79FSncNIws8J43KnO46RhZoXxCLedx0nDzAqzuaYx1TWNTuGkYWaF2TzC7XTXNDqFk4aZFaan2kv3BLHN5O6iQ7GcnDTMrDA91T5mT5uIpKJDsZycNMysMGs39PrKqQ7jpGFmhelZ3+eOfR3GScPMCtNTdU2j0zhpmFlhfC+NzuOkYWaF6an2egiRDuOkYWaFeLpvgI39m5jlNo2O4qRhZoXoSb3BfS+NzuKkYWaF6Fnvcac6USFJQ9IbJK2StEnS4kHrTpe0WtKdko6sW76/pFvTuv8v9wYy62ge4bYzFVXTuA04Fri2fqGkvYClwN7AUcC/S+pKq78KnAjskR5HjVm0ZjbqKrUbMLmm0VEKGfAlIm4Hhho6YAnwvYjYCNwjaTVwoKR7gZkR8ZtU7nzgb4DL2hXj2795I/c9Xm3X7s3GvSefrp2eck2jk5RtlLCdgevr5tekZX1pevDyIUk6kaxWwi677DKiQHbZdjqTut3kY9ZOO86ayvNmTi46DGtB25KGpKuAHYZYdUZE/LhRsSGWRZPlQ4qIZcAygMWLFzfcrpmP/vVeIylmZrZVa1vSiIgjRlBsDbCgbn4+8GBaPn+I5WZmNobKdv7lEmCppMmSdiVr8L4hIh4C1kk6KF019Q9Ao9qKmZm1SVGX3B4jaQ3wMuC/JF0OEBGrgAuB3wE/A06OiIFU7CTgHGA1cBdtbAQ3M7OhKWJEp/w7xuLFi2P58uVFh2Fm1lEkrYiIxYOXl+30lJmZlZiThpmZ5eakYWZmuTlpmJlZblt9Q7ikx4D7Rlh8LvCnMSznY/qYPqaPOZpln4vnR8S8LZZGhB8NHsDysSznY/qYPqaPOZpl2/Hw6SkzM8vNScPMzHJz0mhu2RiX8zF9TB/TxxzNsqNuq28INzOz0eOahpmZ5eakYWZmuTlpDEHSUZLulLRa0odaKPcNSY9Kuq3F4y2Q9EtJt0taJem9LZSdIukGSStT2U+0eOwuSf8j6SctlrtX0q2SbpaUe0RISbMl/aekO9LzfVnOcnumY9UeT0o6NWfZ09Jrc5uk70qa0kK8703lVg13vKHef0nbSrpS0h/S3zk5y70hHXOTpC0GjRum7OfT63uLpIslzc5Z7lOpzM2SrpC0U95j1q17v6SQNDfnMT8u6YG69/W1rRxT0rvT/+oqSZ/Leczv1x3vXkk35z2mpH0lXV/73Es6MGe5l0j6TfqfuVTSzCHKDfk9kOczNKaKvua3bA+gi2zo9d2AScBKYK+cZQ8G9gNua/GYOwL7peltgN+3cEwBM9L0ROC3wEEtHPt9wHeAn7QY873A3BG8vt8E3p6mJwGzR/gePUzW+Wi4bXcG7gGmpvkLgbfmPM4+wG3ANLIbll0F7NHK+w98DvhQmv4Q8Nmc5V4E7AlcDSxu8ZivBrrT9GdbOObMuun3AGe38jknu4Ha5WSdabf4bDQ45seB9+d4L4Yqe2h6Tyan+e3zxlq3/ovAR1s45hXAa9L0a4Grc5a7EXhVmj4B+NQQ5Yb8HsjzGRrLh2saWzoQWB0Rd0dEL/A9YEmeghFxLfBEqweMiIci4qY0vQ64nSb3QB9UNiLiqTQ7MT1yXd0gaT7wV2T3KWm79OvqYOBcgIjojYjKCHZ1OHBXROTt6d8NTJXUTZYA8t718UXA9RFRjYh+4BrgmEYbN3j/l5AlStLfv8lTLiJuj4g7hwuwQdkrUrwA1/Psu142K/dk3ex0GnyOmnzOvwR8cATlhtWg7EnAZyJiY9rm0VaOKUnAG4HvtnDMAGq1hFkM8VlqUG5P4No0fSXwf4Yo1+h7YNjP0Fhy0tjSzsD9dfNryPkFPhokLQReSlZjyFumK1WxHwWujIi8Zb9M9k++qbUogeyf5wpJKySdmLPMbsBjwH+kU2LnSJo+gmMvpcE/+hZBRjwAfAH4I/AQsDYirsh5nNuAgyVtJ2ka2S/LBcOUGex5kd15kvR3+xbLP1cn0MINyySdKel+4M3AR1sodzTwQESsbD1ETkmnxb7R4qmXRcArJf1W0jWSDmjxuK8EHomIP7RQ5lTg8+k1+gJwes5ytwFHp+k3MMznaND3QNGfoWdx0tiShlg2JtclS5oB/BA4ddCvvqYiYiAi9iX7RXmgpH1yHOt1wKMRsWKE4f5FROwHvAY4WdLBOcp0k1XbvxoRLwXWk1W3c5M0ieyf7wc5t59D9kttV2AnYLqkv89TNiJuJzu9cyXZnSRXAv1NC5WIpDPI4r0gb5mIOCMiFqQyp+Q8zjTgDFpIMnW+CuwO7EuW1L/YQtluYA5wEPAB4MJUe8jrb8n546POScBp6TU6jVRrzuEEsv+TFWSnnnobbTjS74Gx4qSxpTU8+1fAfPKfzhgxSRPJPigXRMRFI9lHOtVzNXBUjs3/Ajha0r1kp+AOk/TtFo71YPr7KHAx2Wm94awB1tTVhP6TLIm04jXATRHxSM7tjwDuiYjHIqIPuAh4ed6DRcS5EbFfRBxMdsqhlV+lAI9I2hEg/d3iFEo7SDoOeB3w5kgnw1v0HYY4hdLA7mRJeWX6PM0HbpK0w3AFI+KR9KNnE/B18n2OatYAF6VTtDeQ1Zi3aIAfSjpVeSzw/RaOB3Ac2WcIsh8uueKNiDsi4tURsT9ZorqrQVxDfQ8U8hlqxEljSzcCe0jaNf2qXQpc0s4Dpl9H5wK3R8RZLZadV7s6RtJUsi/JO4YrFxGnR8T8iFhI9hx/ERG5foFLmi5pm9o0WcPrsFeMRcTDwP2S9kyLDie7H3wrWv11+EfgIEnT0ut8ONm54lwkbZ/+7kL2JdPqL9NLyL5oSH9/3GL5lkk6Cvhn4OiIqLZQbo+62aPJ8TkCiIhbI2L7iFiYPk9ryBp0H85xzB3rZo8hx+eozo+Aw9J+FpFdWJF3NNgjgDsiYk0Lx4PsB+Sr0vRh5PwRUfc5mgB8BDh7iG0afQ+M+WeoqSJb4cv6IDt3/XuyXwNntFDuu2RV7D6yf5y35Sz3CrJTYLcAN6fHa3OWfTHwP6nsbTS4EmSYfRxCC1dPkbVNrEyPVS2+RvsCy1O8PwLmtFB2GvA4MKvF5/cJsi/A24Bvka62yVn2V2SJbSVweKvvP7Ad8HOyL5efA9vmLHdMmt4IPAJc3sIxV5O1y9U+S1tcBdWg3A/Ta3QLcCmw80g+5zS4sq7BMb8F3JqOeQmwYwvPcxLw7RTzTcBheWMFzgPeOYL38xXAivR5+C2wf85y7yX7Tvk98BnSaByDyg35PZDnMzSWDw8jYmZmufn0lJmZ5eakYWZmuTlpmJlZbk4aZmaWm5OGmZnl5qRhlpOkp9LfhZL+bpT3/eFB8/89mvs3Gy1OGmatWwi0lDQkdQ2zybOSRkTk7rVuNpacNMxa9xmygfJuVnavji5l97C4MQ289w4ASYek+yN8h6wDG5J+lAZ5XFUb6FHSZ8hG4b1Z0gVpWa1Wo7Tv29K9GN5Ut++r9cy9SS5ocdwlsxHpLjoAsw70IbJ7QLwOIH35r42IAyRNBn4tqTaS7oHAPhFxT5o/ISKeSEO+3CjphxHxIUmnRDbo5GDHkvWifwnZuEo3SqoNsf1SYG+yoS1+TTae2HWj/WTN6rmmYfbcvRr4hzQ8/W/Jhn2ojeN0Q13CAHiPpJVk97lYULddI68AvhvZoH6PkN3TozYE+A0RsSaywf5uJjttZtZWrmmYPXcC3h0Rlz9roXQI2fDv9fNHAC+LiKqkq4Hhbj3b7JTTxrrpAfz/bGPANQ2z1q0juydCzeXASWlYayQtanBzqVlAT0oYLyS7D0RNX638INcCb0rtJvPI7nx4w6g8C7MR8C8Ts9bdAvSn00znAV8hOzV0U2qMfoyhb8n5M+Cdkm4B7iQ7RVWzDLhF0k0R8ea65RcDLyMbVTWAD0bEwynpmI05j3JrZma5+fSUmZnl5qRhZma5OWmYmVluThpmZpabk4aZmeXmpGFmZrk5aZiZWW7/C9uKD/MB9x2bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results.\n",
    "plt.plot(value_iteration_rewards)\n",
    "plt.title('Value Iteration Performance')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.xticks(ticks=np.arange(0,iterations))\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

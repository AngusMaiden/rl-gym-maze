{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "determined-bernard",
   "metadata": {},
   "source": [
    "## SIT796 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-adult",
   "metadata": {},
   "source": [
    "Distinction Task 2.2 - Exact Policy Iteration Implementation for MDPs  \n",
    "**Angus Maiden** | ID 220595465"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-casting",
   "metadata": {},
   "source": [
    "This task implements the policy iteration method of finding an optimal policy that solves a Markov Decision Process (MDP), as outlined in Task 3.1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "editorial-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Invoke the model maze environment  from Task 1.2C.\n",
    "env = gym.make('rl_gym_maze:rl-gym-maze-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-combination",
   "metadata": {},
   "source": [
    "### Initialising variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "played-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise environment.\n",
    "env.reset()\n",
    "\n",
    "# Initialise dictionary to store flattened index of (x,y) coordinates.\n",
    "states = {}\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        states[i, j] = count\n",
    "        count+=1\n",
    "\n",
    "# Initialise V table, max_iterations and gamma.\n",
    "V = np.zeros(100)\n",
    "max_iterations = 1000\n",
    "gamma = 1\n",
    "\n",
    "# Initialise other variables.\n",
    "next_state = np.zeros([4,2])\n",
    "next_reward = np.zeros(4)\n",
    "next_done = np.zeros(4, dtype=bool)\n",
    "next_V = np.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-pattern",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protective-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation():\n",
    "    \n",
    "    # Initalise episode counter.\n",
    "    episodes = 0\n",
    "    \n",
    "    # Policy evluation limited to 1000 sweeps (limited to save computational time).\n",
    "    for i in range(max_iterations):\n",
    "    \n",
    "        # Initialise delta.\n",
    "        delta = 0\n",
    "        \n",
    "        # Initalise state.\n",
    "        env.reset()\n",
    "        \n",
    "        # Iterate though each state.\n",
    "        for x in range(10):\n",
    "            for y in range(10):\n",
    "                 \n",
    "                # Get flattened current state.\n",
    "                current_state = states[x, y]\n",
    "                \n",
    "                # Initalise v.\n",
    "                v = 0\n",
    "            \n",
    "                # Loop over each action.\n",
    "                for a in range(4):\n",
    "                \n",
    "                    # Populate a list of successor states and rewards for each eaction.\n",
    "                    next_state[a], next_reward[a], next_done[a], info = env.step(a)\n",
    "                    next_V[a] = states[next_state[a][0], next_state[a][1]]\n",
    "                \n",
    "                    # Calculate V(s) using Bellman equation:\n",
    "                    v += 0.25 * (next_reward[a] + gamma * V[int(next_V[a])])\n",
    "                \n",
    "                    # This is a lookahead, so reset state to before the step.\n",
    "                    env.location = (x,y)\n",
    "            \n",
    "                # Get delta value, which is the change in V(s).\n",
    "                delta = max(delta, np.abs(V[current_state] - v))\n",
    "            \n",
    "                # Update V(s) with calculated value.\n",
    "                V[current_state] = v\n",
    "            \n",
    "                # The state value of the terminal state is always 0.\n",
    "                V[99] = 0\n",
    "                \n",
    "                # Update environment state to next in sweep.\n",
    "                env.location = (x,y+1)\n",
    "                if env.location[1] == 10:\n",
    "                    env.location = (x+1, 0)\n",
    "                \n",
    "        # Increment episode counter and display progress indicator.\n",
    "        episodes += 1\n",
    "        print(f'\\rPolicy Evaluation: Episode {episodes}', end='')\n",
    "        \n",
    "        # If the change is state value is sufficiently small, exit the loop.\n",
    "        if delta < 0.1:\n",
    "            # Print the results and return the new V table.\n",
    "            print(f'\\rPolicy evaluated in {episodes} episodes.')\n",
    "            return V\n",
    "    \n",
    "    # Print the results and return the new V table.\n",
    "    print(f'\\rPolicy evaluated in {episodes} episodes.')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-survey",
   "metadata": {},
   "source": [
    "### One step look-ahead function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "finished-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_values(state):\n",
    "    \n",
    "    # Check that 'state' argument is a tuple, for environment compatability.\n",
    "    assert isinstance(state, tuple), '\"state\" argument should be a tuple'\n",
    "    \n",
    "    # Initialise Q.\n",
    "    Q = np.zeros(4)\n",
    "    \n",
    "    # Set environment state to 'state' argument.\n",
    "    env.location = state\n",
    "    \n",
    "    # Loop over each action.\n",
    "    for a in range(4):\n",
    "           \n",
    "        # Populate a list of successor states and rewards for each eaction.\n",
    "        next_state[a], next_reward[a], next_done[a], info = env.step(a)\n",
    "        next_V[a] = states[next_state[a][0], next_state[a][1]]\n",
    "                \n",
    "        # Update Q(s,a) for each a using Bellman equation:\n",
    "        Q[a] = (next_reward[a] + gamma * V[int(next_V[a])])\n",
    "                \n",
    "        # This is a lookahead, so reset state to before the step.\n",
    "        env.location = state\n",
    "    \n",
    "    # Return a 4-tuple of the action-values at 'state'.\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-weapon",
   "metadata": {},
   "source": [
    "### Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "significant-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 1000 episodes.\n",
      "\n",
      "Policy Improvement: Iteration 1\n",
      "\n",
      "Policy evaluated in 1000 episodes.\n",
      "\n",
      "Policy Improvement: Iteration 2\n",
      "\n",
      "Policy evaluated in 1000 episodes.\n",
      "\n",
      "Policy Improvement: Iteration 3\n",
      "\n",
      "Policy evaluated in 1000 episodes.\n",
      "\n",
      "Policy Improvement: Iteration 4\n",
      "\n",
      "Policy evaluated in 1000 episodes.\n",
      "\n",
      "Policy Improvement: Iteration 5\n",
      "\n",
      "Policy evaluated in 1000 episodes.\n",
      "\n",
      "Policy Improvement: Iteration 6\n",
      "\n",
      "Found optimal policy in 6 iterations.\n",
      "\n",
      "Grid view of the optimal policy:\n",
      "\n",
      "[[-1380.39 -1376.4  -1368.41 -1356.43 -1340.46 -1323.66 -1294.89 -1254.12 -1230.25 -1198.88]\n",
      " [-1505.85 -1512.17 -1514.49 -1512.82 -1337.3  -1331.65 -1302.87 -1233.25 -1233.75 -1163.52]\n",
      " [-1495.54 -1467.89 -1489.52 -1507.16 -1330.13 -1335.65 -1306.87 -1207.87 -1128.66 -1124.17]\n",
      " [-1481.25 -1442.26 -1519.14 -1515.15 -1318.98 -1322.97 -1194.48 -1178.5  -1129.16 -1076.33]\n",
      " [-1462.95 -1412.65 -1379.04 -1341.44 -1299.84 -1087.81 -1206.46 -1214.45 -1218.44  -971.65]\n",
      " [-1440.67 -1452.65 -1460.63 -1464.63 -1235.11 -1083.81 -1046.64 -1005.47  -960.3   -862.97]\n",
      " [-1402.41 -1243.93 -1222.08 -1196.22 -1166.38 -1113.   -1084.81 -1052.63 -1008.46  -652.98]\n",
      " [-1360.17 -1261.8  -1273.78 -1281.77 -1117.18 -1064.   -1064.61 -1060.62  -184.47  -438.98]\n",
      " [-1313.92 -1263.69 -1211.35 -1285.76 -1012.59 -1006.81  -644.89  -491.42  -180.47  -220.99]\n",
      " [-1166.99 -1163.   -1155.01 -1086.69 -1014.38  -939.85  -794.37  -333.94  -172.47     0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Define initial policy, P, as random (p(s,a) = 0.25 for all s).\n",
    "P = np.ones([100, 4]) / 4\n",
    "\n",
    "# Initialise iterations counter.\n",
    "iterations = 0\n",
    "\n",
    "# Policy iteration limited to 100 iterations (hoping to optimise before limit).\n",
    "for i in range(100):\n",
    "    \n",
    "    # Initialise 'stable' value for determining if policy is optmised.\n",
    "    stable = True\n",
    "    \n",
    "    # Get V table from policy evaluation.\n",
    "    V = policy_evaluation()\n",
    "    \n",
    "    # Iterate though each state.\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "            \n",
    "            # Get flattened current state.\n",
    "            current_state = states[x, y]\n",
    "            \n",
    "            # Select the best action (argmax) under current policy, breaking ties randomly.\n",
    "            old_action = int(np.random.choice(np.flatnonzero(P[current_state] == P[current_state].max())))\n",
    "            \n",
    "            # Get next action values at current state.\n",
    "            Q = action_values((x,y))\n",
    "            \n",
    "            # Select best action (argmax) based on new action values, breaking ties randomly.\n",
    "            new_action = int(np.random.choice(np.flatnonzero(Q == Q.max())))\n",
    "            \n",
    "            # Check if action under the new policy is different than the old.\n",
    "            if old_action != new_action:\n",
    "                stable = False\n",
    "                P[current_state] = np.eye(4)[new_action]\n",
    "            \n",
    "            # Update environment state to next in sweep.\n",
    "            env.location = (x,y+1)\n",
    "            if env.location[1] == 10:\n",
    "                env.location = (x+1, 0)\n",
    "    \n",
    "    # Increment iteration counter and display progress indicator.\n",
    "    iterations += 1\n",
    "    print(f'\\nPolicy Improvement: Iteration {iterations}\\n')\n",
    "    \n",
    "    # If no change in policy, we have an optimal policy (exit the loop).\n",
    "    if stable:\n",
    "        break\n",
    "\n",
    "# Print the results, including optimal state values as an array.\n",
    "print(f'Found optimal policy in {iterations} iterations.\\n')\n",
    "print(f'Grid view of the optimal policy:\\n')\n",
    "np.set_printoptions(precision = 2, suppress = True, linewidth = 100)\n",
    "print(V.reshape([10,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-holiday",
   "metadata": {},
   "source": [
    "### Running an episode with optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stock-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 | Location: [0 1] | Reward received: -1\n",
      "Step: 2 | Location: [0 2] | Reward received: -1\n",
      "Step: 3 | Location: [0 3] | Reward received: -1\n",
      "Step: 4 | Location: [0 4] | Reward received: -1\n",
      "Step: 5 | Location: [0 5] | Reward received: -1\n",
      "Step: 6 | Location: [0 6] | Reward received: -1\n",
      "Step: 7 | Location: [0 7] | Reward received: -1\n",
      "Step: 8 | Location: [0 8] | Reward received: -1\n",
      "Step: 9 | Location: [0 9] | Reward received: -1\n",
      "Step: 10 | Location: [1 9] | Reward received: -1\n",
      "Step: 11 | Location: [2 9] | Reward received: -1\n",
      "Step: 12 | Location: [3 9] | Reward received: -1\n",
      "Step: 13 | Location: [4 9] | Reward received: -1\n",
      "Step: 14 | Location: [5 9] | Reward received: -1\n",
      "Step: 15 | Location: [6 9] | Reward received: -1\n",
      "Step: 16 | Location: [7 9] | Reward received: -1\n",
      "Step: 17 | Location: [8 9] | Reward received: -1\n",
      "Step: 18 | Location: [9 9] | Reward received: 0\n",
      "\n",
      "Reached the goal in 18 steps with optimal policy.\n",
      "Total episode reward: -17\n"
     ]
    }
   ],
   "source": [
    "# Initalise environment and other variables.\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0\n",
    "total_reward = 0\n",
    "\n",
    "# Limit episode to 1000 steps (hoping to finish the task before limit).\n",
    "for step in range(1000):\n",
    "    \n",
    "     # Get flattened current state.\n",
    "    current_state = states[state[0],state[1]]\n",
    "    \n",
    "    # Select next action based on optimal policy.\n",
    "    action = int(np.random.choice(np.flatnonzero(P[current_state] == P[current_state].max())))\n",
    "    \n",
    "    # Get state, reward and done variables from environment 'step' function.\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update counter and reward tracking variables.\n",
    "    counter += 1\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Display current location as agent moves through maze.\n",
    "    print(f'Step: {counter} | Location: {state} | Reward received: {reward}')\n",
    "    \n",
    "    # If we reach the goal, end the episode and display results.\n",
    "    if done:\n",
    "        print(f'\\nReached the goal in {counter} steps with optimal policy.')\n",
    "        print(f'Total episode reward: {total_reward}')\n",
    "        break\n",
    "    \n",
    "    if step == 999:\n",
    "        print(f'Failed to reach goal within limit. Not an optimal policy.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
